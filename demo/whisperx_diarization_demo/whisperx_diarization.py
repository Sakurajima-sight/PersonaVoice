import whisperx
import torch
import gc
import os
import json
import mmap
import pandas as pd
from io import StringIO
import argparse
import ffmpeg
from concurrent.futures import ThreadPoolExecutor
from datetime import timedelta
import pyjson5


# ------------------ æ¨¡å‹æ¨ç† ------------------
SUPPORTED_LANG_CODES = ["en", "fr", "de", "es", "it", "ja", "zh", "nl", "uk", "pt", "yue"]

def run_whisperx_pipeline(audio_file, language_code, whisper_model, device, output_dir,
                          tf32, gpu_mem_fraction, batch_size, compute_type):
    if tf32:
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True

    torch.cuda.set_per_process_memory_fraction(gpu_mem_fraction, device=0)
    os.makedirs(output_dir, exist_ok=True)

    print("ğŸ” åŠ è½½ WhisperX æ¨¡å‹...")
    model = whisperx.load_model(whisper_model, device, language=language_code, compute_type=compute_type, download_root="./path")
    audio = whisperx.load_audio(audio_file)
    result = model.transcribe(audio, batch_size=batch_size)
    del model; gc.collect(); torch.cuda.empty_cache()

    print("ğŸ” å¯¹é½æ–‡æœ¬...")
    model_a, metadata = whisperx.load_align_model(language_code=language_code, device=device)
    result = whisperx.align(result["segments"], model_a, metadata, audio, device, return_char_alignments=True)
    del model_a; gc.collect(); torch.cuda.empty_cache()

    print("ğŸ” è¯´è¯äººåˆ†ç¦»ä¸­...")
    # åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
    from dotenv import load_dotenv
    load_dotenv()
    huggingface_token = os.getenv("HUGGINGFACE_TOKEN")

    diarize_model = whisperx.DiarizationPipeline(
        use_auth_token=huggingface_token,  device=device
    )
    diarize_segments = diarize_model(audio)
    result = whisperx.assign_word_speakers(diarize_segments, result)

    # å›ºå®šæ–‡ä»¶å
    diarize_path = os.path.join(output_dir, "diarize_segments.txt")
    result_path = os.path.join(output_dir, "result_with_speakers.txt")

    pd.set_option('display.max_rows', None)
    with open(diarize_path, "w", encoding="utf-8") as f:
        f.write(str(diarize_segments))
    print(f"âœ… è¯´è¯äººç‰‡æ®µå·²ä¿å­˜ï¼š{diarize_path}")

    with open(result_path, "w", encoding="utf-8") as f:
        f.write(str(result["segments"]))
    print(f"âœ… å¸¦è¯´è¯äººè½¬å½•å·²ä¿å­˜ï¼š{result_path}")

    return diarize_path, result_path

# ------------------ æ–‡ä»¶å·¥å…· ------------------
def process_result_with_speakers(input_file, output_dir):
    if not os.path.exists(input_file):
        print("âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨")
        return

    # è¯»å–åŸå§‹æ–‡ä»¶å†…å®¹
    with open(input_file, "r", encoding="utf-8") as f:
        mmapped_file = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
        content = mmapped_file.read().decode('utf-8')

    try:
        # ä½¿ç”¨ pyjson5 æ›¿ä»£ demjson3ï¼Œè§£æä¸º Python å¯¹è±¡
        parsed_data = pyjson5.decode(content)

        # ä½¿ç”¨æ ‡å‡† json åº“å†™å‡ºåˆæ³• JSON æ–‡ä»¶ï¼ˆæ›´å¿«ï¼‰
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, "result_with_speakers.json")
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(parsed_data, f, ensure_ascii=False, indent=2)

        os.remove(input_file)
        print(f"âœ… å¤„ç†åçš„ JSON è¾“å‡ºï¼š{output_file}ï¼ˆå¹¶å·²åˆ é™¤åŸæ–‡ä»¶ï¼‰")

    except Exception as e:
        print(f"âŒ JSON è§£æå¤±è´¥ï¼š{e}")


def process_diarize_segments(input_file, output_dir):
    if not os.path.exists(input_file): return
    with open(input_file, 'r') as f:
        content = "\n".join([line.strip() for line in f.read().strip().splitlines()])
    data = StringIO(content)
    df = pd.read_csv(data, delim_whitespace=True)
    if 'segment' in df.columns:
        df = df.drop(columns=['segment'])

    output_file = os.path.join(output_dir, "diarize_segments.csv")
    df.to_csv(output_file, index=False)
    os.remove(input_file)
    print(f"âœ… å¤„ç†åçš„ CSV è¾“å‡ºï¼š{output_file}ï¼ˆå¹¶å·²åˆ é™¤åŸæ–‡ä»¶ï¼‰")

def create_file_list(directory):
    wav_files = [
        f for f in os.listdir(directory)
        if f.endswith('.wav') and not f.endswith('_merged_collection.wav')
    ]
    if not wav_files:
        print("æ²¡æœ‰æ‰¾åˆ° WAV æ–‡ä»¶ï¼ˆå·²æ’é™¤åˆå¹¶æ–‡ä»¶ï¼‰")
        return None
    file_list_path = os.path.join(directory, 'file_list.txt')
    with open(file_list_path, 'w', encoding='utf-8') as f:
        for wav_file in wav_files:
            f.write(f"file '{wav_file}'\n")
    return file_list_path


def merge_wavs_ffmpeg(file_list_path, speaker_id, directory):
    if file_list_path is None:
        print("æ²¡æœ‰æä¾›æœ‰æ•ˆçš„æ–‡ä»¶åˆ—è¡¨è·¯å¾„")
        return
    output_filename = os.path.join(directory, f"{speaker_id}_merged_collection.wav")
    try:
        ffmpeg.input(file_list_path, format='concat', safe=0) \
              .output(output_filename, c='copy') \
              .run(overwrite_output=True, capture_stdout=True, capture_stderr=True)
        print(f"åˆå¹¶å®Œæˆï¼š{output_filename}")
    except ffmpeg.Error as e:
        print(f"åˆå¹¶å¤±è´¥ï¼š{e}")
        return
    # åˆ é™¤ä¸´æ—¶åˆ†æ®µéŸ³é¢‘
    with open(file_list_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip().startswith("file '") and line.strip().endswith("'"):
                filename = line.strip()[6:-1]
                file_path = os.path.join(directory, filename)
                if os.path.exists(file_path):
                    os.remove(file_path)
    os.remove(file_list_path)


# ------------------ æ•°æ®è§£æ ------------------
def parse_diarization_file(diarization_file):
    df = pd.read_csv(diarization_file)
    segments = []
    for _, row in df.iterrows():
        start = row['start']
        end = row['end']
        speaker = row['speaker']
        if (end - start) >= 0.0:
            segments.append((start, end, speaker))
    return segments

def extract_audio_segment(wav_file, start_time, end_time, temp_file):
    duration = end_time - start_time
    ffmpeg.input(wav_file, ss=start_time).output(
        temp_file,
        t=duration,
        acodec='pcm_s16le'
    ).run(overwrite_output=True, capture_stdout=True, capture_stderr=True)

def extract_and_merge_speaker_audio(wav_file, diarization_file, speaker, output_folder, max_threads=4):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    segments = parse_diarization_file(diarization_file)
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = []
        for i, (start, end, spk) in enumerate(segments):
            if spk == speaker:
                temp_file = os.path.join(output_folder, f"{speaker}_segment_{i+1}.wav")
                futures.append(executor.submit(extract_audio_segment, wav_file, start, end, temp_file))
        for future in futures:
            future.result()
    file_list_path = create_file_list(output_folder)
    merge_wavs_ffmpeg(file_list_path, speaker, output_folder)

# ------------------ ç»Ÿè®¡å’Œä¸»å…¥å£ ------------------
def get_speaker_durations(diarization_file):
    df = pd.read_csv(diarization_file)
    durations = {}
    for _, row in df.iterrows():
        start = row['start']
        end = row['end']
        speaker = row['speaker']
        duration = end - start
        durations[speaker] = durations.get(speaker, 0) + duration
    return durations

def seconds_to_hms(seconds):
    return str(timedelta(seconds=int(seconds)))

def main():
    parser = argparse.ArgumentParser(description="WhisperX è½¬å½• + è¯´è¯äººåˆ†ç¦»å·¥å…·")

    # å¿…éœ€å‚æ•°
    parser.add_argument("audio_file", help="éŸ³é¢‘æ–‡ä»¶è·¯å¾„")

    # å¯é€‰å‚æ•°
    parser.add_argument("--language_code", default="en", help="è¯­è¨€ä»£ç ï¼ˆé»˜è®¤ï¼šenï¼‰")
    parser.add_argument("--model", default="large-v2", help="ä½¿ç”¨çš„ Whisper æ¨¡å‹åç§°ï¼ˆé»˜è®¤ large-v2ï¼‰")
    parser.add_argument("--output_dir", default="./diar_segments_whisperx", help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤å½“å‰ç›®å½•ï¼‰")
    parser.add_argument("--tf32", action="store_true", help="å¯ç”¨ TF32 åŠ é€Ÿï¼ˆé»˜è®¤å…³é—­ï¼‰")
    parser.add_argument("--gpu_mem_fraction", type=float, default=0.5, help="å•è¿›ç¨‹ä½¿ç”¨çš„ GPU æ˜¾å­˜å æ¯”ï¼ˆé»˜è®¤ 0.5ï¼‰")
    parser.add_argument("--batch_size", type=int, default=4, help="æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ 4ï¼‰")
    parser.add_argument("--compute_type", default="float16", help="è®¡ç®—ç±»å‹ï¼šfloat16, int8 ç­‰ï¼ˆé»˜è®¤ float16ï¼‰")
    parser.add_argument("--top_k_speakers", type=int, default=None, help="å¯¼å‡ºè¯´è¯æ—¶é•¿æœ€å¤šçš„å‰ K ä¸ªè¯´è¯äººï¼ˆé»˜è®¤å…¨éƒ¨å¯¼å‡ºï¼‰")
    parser.add_argument("--max_threads", type=int, default=4, help="å¹¶å‘æå–éŸ³é¢‘æ—¶çš„çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ 4ï¼‰")


    args = parser.parse_args()

    if args.language_code not in SUPPORTED_LANG_CODES:
        print(f"âŒ ä¸æ”¯æŒçš„ language_code: {args.language_code}")
        print(f"âœ… æ”¯æŒè¯­è¨€ä»£ç : {SUPPORTED_LANG_CODES}")
        return

    diarize_path, result_path = run_whisperx_pipeline(
        audio_file=args.audio_file,
        language_code=args.language_code,
        whisper_model=args.model,
        device="cuda",
        output_dir=args.output_dir,
        tf32=args.tf32,
        gpu_mem_fraction=args.gpu_mem_fraction,
        batch_size=args.batch_size,
        compute_type=args.compute_type
    )

    process_diarize_segments(diarize_path, args.output_dir)
    process_result_with_speakers(result_path, args.output_dir)

    # è·å–è¯´è¯äººæ—¶é•¿
    output_csv = os.path.join(args.output_dir, "diarize_segments.csv")
    durations = get_speaker_durations(output_csv)
    sorted_speakers = sorted(durations.items(), key=lambda x: x[1], reverse=True)

    if args.top_k_speakers:
        selected_speakers = sorted_speakers[:args.top_k_speakers]
    else:
        selected_speakers = sorted_speakers

    print("\nğŸ“¦ æ­£åœ¨æå–ä»¥ä¸‹è¯´è¯äººéŸ³é¢‘ï¼š")
    for speaker, duration in selected_speakers:
        print(f"ğŸ‘¤ {speaker}: {seconds_to_hms(duration)}")

    for speaker, _ in selected_speakers:
        extract_and_merge_speaker_audio(
            wav_file=args.audio_file,
            diarization_file=output_csv,
            speaker=speaker,
            output_folder=args.output_dir,
            max_threads=args.max_threads
        )
    print(f"ğŸ™ï¸  è¯´è¯äºº {speaker} çš„éŸ³é¢‘å·²åˆå¹¶å¯¼å‡ºåˆ°ï¼š{args.output_dir}")

    total_duration = sum(d for _, d in selected_speakers)
    print(f"\nğŸ§® æ€»å¯¼å‡ºè¯´è¯æ—¶é•¿ï¼š{seconds_to_hms(total_duration)}")

if __name__ == "__main__":
    main()
