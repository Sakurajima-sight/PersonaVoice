import os
import numpy as np
import torch
import argparse
from nemo.collections.asr.models import SortformerEncLabelModel
import csv
import ffmpeg
import shutil
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
from datetime import timedelta

# ------------------ æ¨¡å‹æ¨ç† ------------------
def perform_inference(audio_files_list, batch_size=1):
    """
    å…ˆè¿›è¡ŒéŸ³é¢‘åˆ†å‰²ï¼Œç„¶åä½¿ç”¨æä¾›çš„æ¨¡å‹å¯¹åˆ†å‰²åçš„éŸ³é¢‘è¿›è¡Œè¯´è¯äººåˆ†ç¦»ã€‚

    å‚æ•°ï¼š
    - diar_model: ç”¨äºæ¨ç†çš„è¯´è¯äººåˆ†ç¦»æ¨¡å‹ã€‚
    - audio_path: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆWAVï¼‰ã€‚
    - max_segment_duration: æ¯ä¸ªéŸ³é¢‘ç‰‡æ®µçš„æœ€å¤§æ—¶é•¿ï¼ˆç§’ï¼‰ã€‚
    - batch_size: æ¯æ¬¡å¤„ç†çš„éŸ³é¢‘æ–‡ä»¶æ•°ï¼ˆé»˜è®¤1ï¼‰ã€‚

    è¿”å›ï¼š
    - predicted_segments: é¢„æµ‹çš„è¯´è¯äººåˆ†ç¦»ç»“æœã€‚
    - tensor_outputs: æ¨¡å‹çš„å¼ é‡è¾“å‡ºï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰ã€‚
    """
    torch.cuda.empty_cache()
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # åŠ è½½æ¨¡å‹
    diar_model = SortformerEncLabelModel.from_pretrained("nvidia/diar_sortformer_4spk-v1")
    diar_model.eval()
    diar_model = diar_model.to(device)

    # æ¨ç†
    print("ğŸš€ æ­£åœ¨è¿›è¡Œè¯´è¯äººåˆ†ç¦»...")
    predicted_segments, _ = diar_model.diarize(
        audio=audio_files_list,
        batch_size=batch_size,
        include_tensor_outputs=True
    )

    # è¿”å›ç»“æœåŠä¸´æ—¶ç›®å½•ä»¥ä¾¿æ¸…ç†
    return predicted_segments


# ------------------ æ–‡ä»¶å·¥å…· ------------------
def split_audio(audio_path, max_duration):
    temp_dir = os.path.join(os.getcwd(), 'temp_wav')
    os.makedirs(temp_dir, exist_ok=True)

    probe = ffmpeg.probe(audio_path, v='error', select_streams='a', show_entries='format=duration')
    total_duration = float(probe['format']['duration'])
    num_segments = int(np.ceil(total_duration / max_duration))
    
    audio_files = []

    for i in range(num_segments):
        start_time = i * max_duration
        segment_filename = os.path.join(temp_dir, f"segment_{i+1}.wav")
        ffmpeg.input(audio_path, ss=start_time, t=max_duration) \
            .output(segment_filename) \
            .run(overwrite_output=True, capture_stdout=True, capture_stderr=True)
        audio_files.append(segment_filename)
    
    return audio_files, temp_dir

def save_diarization_results_to_csv(segments, output_filepath):
    os.makedirs(os.path.dirname(output_filepath), exist_ok=True)

    with open(output_filepath, mode='w', newline='') as csv_file:
        csv_writer = csv.writer(csv_file)
        csv_writer.writerow(['start', 'end', 'speaker'])

        for segment_list in segments:
            for segment in segment_list:
                parts = segment.split()
                begin = float(parts[0])
                end = float(parts[1])
                speaker = str(parts[2])
                csv_writer.writerow([begin, end, speaker])
    
    print(f"âœ… Diarization results saved to {output_filepath}")


def create_file_list(directory):
    wav_files = [
        f for f in os.listdir(directory)
        if f.endswith('.wav') and not f.endswith('_merged_collection.wav')
    ]
    if not wav_files:
        print("æ²¡æœ‰æ‰¾åˆ° WAV æ–‡ä»¶ï¼ˆå·²æ’é™¤åˆå¹¶æ–‡ä»¶ï¼‰")
        return None
    file_list_path = os.path.join(directory, 'file_list.txt')
    with open(file_list_path, 'w', encoding='utf-8') as f:
        for wav_file in wav_files:
            f.write(f"file '{wav_file}'\n")
    return file_list_path


def merge_wavs_ffmpeg(file_list_path, speaker_id, directory):
    if file_list_path is None:
        print("æ²¡æœ‰æä¾›æœ‰æ•ˆçš„æ–‡ä»¶åˆ—è¡¨è·¯å¾„")
        return
    output_filename = os.path.join(directory, f"{speaker_id}_merged_collection.wav")
    try:
        ffmpeg.input(file_list_path, format='concat', safe=0) \
              .output(output_filename, c='copy') \
              .run(overwrite_output=True, capture_stdout=True, capture_stderr=True)
        print(f"åˆå¹¶å®Œæˆï¼š{output_filename}")
    except ffmpeg.Error as e:
        print(f"åˆå¹¶å¤±è´¥ï¼š{e}")
        return
    # åˆ é™¤ä¸´æ—¶åˆ†æ®µéŸ³é¢‘
    with open(file_list_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip().startswith("file '") and line.strip().endswith("'"):
                filename = line.strip()[6:-1]
                file_path = os.path.join(directory, filename)
                if os.path.exists(file_path):
                    os.remove(file_path)
    os.remove(file_list_path)

def clean_up_temp_files(temp_dir):
    shutil.rmtree(temp_dir)


# ------------------ æ•°æ®è§£æ ------------------
def parse_diarization_file(diarization_file):
    df = pd.read_csv(diarization_file)
    segments = []
    for _, row in df.iterrows():
        start = row['start']
        end = row['end']
        speaker = row['speaker']
        if (end - start) >= 0.0:
            segments.append((start, end, speaker))
    return segments

def extract_audio_segment(wav_file, start_time, end_time, temp_file):
    duration = end_time - start_time
    ffmpeg.input(wav_file, ss=start_time).output(
        temp_file,
        t=duration,
        acodec='pcm_s16le'
    ).run(overwrite_output=True, capture_stdout=True, capture_stderr=True)

def extract_and_merge_speaker_audio(wav_file, diarization_file, speaker, output_folder, max_threads=4):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    segments = parse_diarization_file(diarization_file)
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = []
        for i, (start, end, spk) in enumerate(segments):
            if spk == speaker:
                temp_file = os.path.join(output_folder, f"{speaker}_segment_{i+1}.wav")
                futures.append(executor.submit(extract_audio_segment, wav_file, start, end, temp_file))
        for future in futures:
            future.result()
    file_list_path = create_file_list(output_folder)
    merge_wavs_ffmpeg(file_list_path, speaker, output_folder)

# ------------------ ç»Ÿè®¡å’Œä¸»å…¥å£ ------------------
def get_speaker_durations(diarization_file):
    df = pd.read_csv(diarization_file)
    durations = {}
    for _, row in df.iterrows():
        start = row['start']
        end = row['end']
        speaker = row['speaker']
        duration = end - start
        durations[speaker] = durations.get(speaker, 0) + duration
    return durations

def seconds_to_hms(seconds):
    return str(timedelta(seconds=int(seconds)))


def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨ NeMo Sortformer å¯¹éŸ³é¢‘è¿›è¡Œè¯´è¯äººåˆ†ç¦»å¹¶å¯¼å‡ºä¸º CSV")

    parser.add_argument('--audio_file', type=str, required=True, help='è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆWAVï¼‰')
    parser.add_argument('--max_segment_duration', type=int, default=300, help='å•ä¸ªç‰‡æ®µæœ€å¤§é•¿åº¦ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤300')
    parser.add_argument("--output_dir", default="./diar_segments_NeMo", help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ç›®å½•ï¼‰")
    parser.add_argument("--top_k_speakers", type=int, default=None, help="å¯¼å‡ºè¯´è¯æ—¶é•¿æœ€å¤šçš„å‰ K ä¸ªè¯´è¯äººï¼ˆé»˜è®¤å…¨éƒ¨å¯¼å‡ºï¼‰")
    parser.add_argument("--max_threads", type=int, default=4, help="å¹¶å‘æå–éŸ³é¢‘æ—¶çš„çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ 4ï¼‰")
    parser.add_argument("--batch_size", type=int, default=1, help="æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ 1ï¼‰")

    args = parser.parse_args()

    probe = ffmpeg.probe(args.audio_file, v='error', select_streams='a:0', show_entries='stream=channels')
    channels = probe['streams'][0]['channels']

    if channels == 2:
        print(f"éŸ³é¢‘ {args.audio_file} æ˜¯åŒé€šé“ï¼Œæ­£åœ¨è½¬æ¢ä¸ºå•é€šé“...")
        output_file = os.path.splitext(args.audio_file)[0] + "_mono.wav"
        ffmpeg.input(args.audio_file).output(output_file, ac=1).run(overwrite_output=True, capture_stdout=True, capture_stderr=True)
        print(f"è½¬æ¢å®Œæˆï¼Œå·²è¦†ç›–åŸæ–‡ä»¶ {args.audio_file}")
    else:
        output_file = args.audio_file

    audio_path = output_file
    max_segment_duration = args.max_segment_duration
    output_csv = os.path.join(args.output_dir, "nvidia_diarization_results.csv")

    print(f"ğŸ§ åˆ†æéŸ³é¢‘ï¼š{audio_path}")
    print(f"â±ï¸ æ¯æ®µæœ€å¤§æ—¶é•¿ï¼š{max_segment_duration} ç§’")
    print(f"ğŸ“„ è¾“å‡ºè·¯å¾„ï¼š{output_csv}")

    # åˆ†å‰²éŸ³é¢‘
    audio_files, temp_dir = split_audio(audio_path, max_segment_duration)

    # æ‰§è¡Œæ¨ç†
    predicted_segments = perform_inference(audio_files, args.batch_size)

    # ä¿å­˜ç»“æœ
    save_diarization_results_to_csv(predicted_segments, output_csv)

    # æ¸…ç†
    clean_up_temp_files(temp_dir)


    # æå–è¯´è¯äºº
    durations = get_speaker_durations(output_csv)
    sorted_speakers = sorted(durations.items(), key=lambda x: x[1], reverse=True)

    if args.top_k_speakers:
        selected_speakers = sorted_speakers[:args.top_k_speakers]
    else:
        selected_speakers = sorted_speakers

    print("\nğŸ“¦ æ­£åœ¨æå–ä»¥ä¸‹è¯´è¯äººéŸ³é¢‘ï¼š")
    for speaker, duration in selected_speakers:
        print(f"ğŸ‘¤ {speaker}: {seconds_to_hms(duration)}")

    for speaker, _ in selected_speakers:
        extract_and_merge_speaker_audio(
            wav_file=args.audio_file,
            diarization_file=output_csv,
            speaker=speaker,
            output_folder=args.output_dir,
            max_threads=args.max_threads
        )
    print(f"ğŸ™ï¸  è¯´è¯äºº {speaker} çš„éŸ³é¢‘å·²åˆå¹¶å¯¼å‡ºåˆ°ï¼š{args.output_dir}")

    total_duration = sum(d for _, d in selected_speakers)
    print(f"\nğŸ§® æ€»å¯¼å‡ºè¯´è¯æ—¶é•¿ï¼š{seconds_to_hms(total_duration)}")

    print("âœ… å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    main() 